{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[],"source":["import numpy as np \n","import pandas as pd\n","import seaborn as sns\n","import matplotlib.pyplot as plt"]},{"cell_type":"markdown","metadata":{},"source":["# Tweets - real or fake disaster\n","* Disaster detection from twitter feed\n","* wild fire, accident, you name it! We will find out whether the tweets is indicative of a real disaster or not\n","\n","## Apporach\n","1. get the data\n","2. inspect the data\n","3. clean up the tweets, stripping, pre-processing\n","    * need to shuffle the train data before splitting it into train and cross validation\n","4. decide what deep learning model to use\n","    * use tensorflow to do tokenization,embedding and dense layers\n","5. improve model using cross validation data\n","6. output the test data\n","7. submit"]},{"cell_type":"markdown","metadata":{},"source":["# Step 1. get the data"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"outputs":[],"source":["# inputting the files\n","test_path = '../input/test.csv'\n","train_path = '../input/train.csv'\n","\n","test = pd.read_csv(test_path, index_col='id')\n","train = pd.read_csv(train_path, index_col='id')"]},{"cell_type":"markdown","metadata":{},"source":["# Step 2. inspect the data"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# test shape - 3263 x 3\n","# train shape - 7613 x 4\n","print(\" test shape: \" ,test.shape)\n","print(\" train shape: \" ,train.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["train.head()"]},{"cell_type":"markdown","metadata":{},"source":["## Columns:\n","* keyword - ponential useful in identifying whether it is a disaster\n","* location - might not be relevant except to try and match the tweets, but would require alot of work\n","* text - the tweets, main input\n","* target - 1 as real disaster, 0 as not a disaster"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["print(\"---% missing in train---\")\n","print(train.isnull().sum()/7613*100)\n","\n","print(\"---% missing in test---\")\n","print(test.isnull().sum()/3263*100)"]},{"cell_type":"markdown","metadata":{},"source":["## Missing Data:\n","* Keyword is mostly present, with <1 % missing, can manifacture keyword from tweets, or set it as 'unknown'\n","* location missing for most, might be better to not use as a feature"]},{"cell_type":"markdown","metadata":{},"source":["## Feature engineering\n","* If we want to use the keyword as a feature, let's check whether certain keywords are indicative of a real disaster or a fake"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# this would set the target_mean as the mean of the target belonging to that keyword\n","train['target_mean'] = train.groupby('keyword')['target'].transform('mean')\n","# check if it is set properly\n","train[train['keyword']=='ablaze']"]},{"cell_type":"markdown","metadata":{},"source":["The code of the graph below is from the notebook \"NLP with Disaster Tweets - EDA, Cleaning and BERT\"\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["## The code of this graph is from the notebook \"NLP with Disaster Tweets - EDA, Cleaning and BERT\"\n","\n","fig1 = plt.figure(figsize=(8, 72), dpi=100)\n","\n","sns.countplot(y=train.sort_values(by='target_mean', ascending=False)['keyword'], hue=train.sort_values(by='target_mean', ascending=False)['target'])\n","plt.tick_params(axis='x', labelsize=12)\n","plt.tick_params(axis='y', labelsize=12)\n","plt.legend(loc='upper right')\n","plt.title('Target Distribution in Keywords')\n","\n","plt.show()\n","\n","## space is shown as %20 - what to do with multiword keywords?"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["train.keyword.unique()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["print(\"# of unique values in train - # of unique values in test:\" , train.keyword.nunique() - test.keyword.nunique())\n","print(set(train['keyword'].unique()) == set(test['keyword'].unique())) # so there are no new keyworkds in test that wasn't in train\n","print(\"# of unique locations in train - # of unique locations in test:\" , train.location.nunique() - test.location.nunique())"]},{"cell_type":"markdown","metadata":{},"source":["## Word counts"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# how many word in the tweets\n","train['word_counts'] = train['text'].apply(lambda x: len(str(x).split()))\n","test['word_counts'] = test['text'].apply(lambda x: len(str(x).split()))\n","# average length of chars in the word\n","train['avg_word_length'] = train['text'].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n","test['avg_word_length'] = test['text'].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n","# how many characters in the tweets\n","train['char_counts'] = train['text'].apply(lambda x: len(str(x)))\n","test['char_counts'] = test['text'].apply(lambda x: len(str(x)))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["train.head()"]},{"cell_type":"markdown","metadata":{},"source":["Looking at the distribution of these new features"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["fig2, axes = plt.subplots(ncols=3, nrows=1, figsize=(20, 10), dpi=100)\n","\n","bins = 100\n","plt.subplot(1, 3, 1)\n","plt.hist(train[train.target == 0]['word_counts'], alpha = 0.6, bins=bins, label='Fake', color='green')\n","plt.hist(train[train.target == 1]['word_counts'], alpha = 0.6, bins=bins, label='Real', color='red')\n","plt.xlabel('word counts')\n","plt.ylabel('count')\n","plt.legend(loc='upper right')\n","plt.subplot(1, 3, 2)\n","plt.hist(train[train.target == 0]['char_counts'], alpha = 0.6, bins=bins, label='Fake', color='green')\n","plt.hist(train[train.target == 1]['char_counts'], alpha = 0.6, bins=bins, label='Real', color='red')\n","plt.xlabel('characters counts')\n","plt.ylabel('count')\n","plt.legend(loc='upper right')\n","plt.subplot(1, 3, 3)\n","plt.hist(train[train.target == 0]['avg_word_length'], alpha = 0.6, bins=bins, label='Fake', color='green')\n","plt.hist(train[train.target == 1]['avg_word_length'], alpha = 0.6, bins=bins, label='Real', color='red')\n","plt.xlabel('average word length')\n","plt.ylabel('count')\n","plt.legend(loc='upper right')\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["Looks like real disaster tweets have :\n","* higher word counts\n","* larger characters counts\n","* higher average word length\n","These 3 are closely related to each other, quite redundent, probably only would use one\n","* average word lengths looks like good candidate sine it is roughly uniform and clearer separation"]},{"cell_type":"markdown","metadata":{},"source":["## Checking for useless information:\n","\n","### Stop words\n","* words that carry no meanings and do not need to be used in the analysis\n","\n","### Punctuation\n","* Unless real disaster use less of a certain puncutation, not useful --- let's check before removing\n","\n","### Links / html tags\n","* Unless real disaster has less links, not useful --- let's check before removing"]},{"cell_type":"markdown","metadata":{},"source":["Defining the stopwords --- stopwords comng from Yoast SEO"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["## definte the stopwords\n","STOPWORDS = [ \"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \"as\", \"at\", \"be\", \"because\", \"been\", \"before\", \"being\", \"below\", \"between\", \"both\", \"but\", \"by\", \"could\", \"did\", \"do\", \"does\", \"doing\", \"down\", \"during\", \"each\", \"few\", \"for\", \"from\", \"further\", \"had\", \"has\", \"have\", \"having\", \"he\", \"he'd\", \"he'll\", \"he's\", \"her\", \"here\", \"here's\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"how's\", \"i\", \"i'd\", \"i'll\", \"i'm\", \"i've\", \"if\", \"in\", \"into\", \"is\", \"it\", \"it's\", \"its\", \"itself\", \"let's\", \"me\", \"more\", \"most\", \"my\", \"myself\", \"nor\", \"of\", \"on\", \"once\", \"only\", \"or\", \"other\", \"ought\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"same\", \"she\", \"she'd\", \"she'll\", \"she's\", \"should\", \"so\", \"some\", \"such\", \"than\", \"that\", \"that's\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"there\", \"there's\", \"these\", \"they\", \"they'd\", \"they'll\", \"they're\", \"they've\", \"this\", \"those\", \"through\", \"to\", \"too\", \"under\", \"until\", \"up\", \"very\", \"was\", \"we\", \"we'd\", \"we'll\", \"we're\", \"we've\", \"were\", \"what\", \"what's\", \"when\", \"when's\", \"where\", \"where's\", \"which\", \"while\", \"who\", \"who's\", \"whom\", \"why\", \"why's\", \"with\", \"would\", \"you\", \"you'd\", \"you'll\", \"you're\", \"you've\", \"your\", \"yours\", \"yourself\", \"yourselves\" ]"]},{"cell_type":"markdown","metadata":{},"source":["Let's check if there is any specific punctuation that is most common"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from collections import defaultdict\n","import string\n","def make_pun_dict(val):\n","    pun_dict = defaultdict(int)\n","    for tweet in train[train.target == int(val)]['text']:\n","        pun = [pun for pun in str(tweet).lower().split() if pun in string.punctuation]\n","        for sym in pun:\n","            pun_dict[sym] +=1\n","    return pun_dict"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["real_pun_dict = make_pun_dict(1)\n","fake_pun_dict = make_pun_dict(0)"]},{"cell_type":"markdown","metadata":{},"source":["Looks like the punctuation counts are very common, let's check the number of punctuation just in case"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["fig3 = plt.figure(figsize=(10, 5), dpi=100)\n","plt.bar(real_pun_dict.keys(), real_pun_dict.values(), color='r', label='Real', alpha=0.6)\n","plt.bar(fake_pun_dict.keys(), fake_pun_dict.values(), color='g', label='Fake', alpha=0.6)\n","plt.xlabel('symbol')\n","plt.ylabel('count')\n","plt.legend(loc='upper right')"]},{"cell_type":"markdown","metadata":{},"source":["The number of punctuations used in real disaster tweet is slighter higher than in fake ones, but not big difference"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["fig4 = plt.figure(figsize=(10, 5), dpi=100)\n","train['pun_counts'] = train['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\n","test['pun_counts'] = test['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\n","bins = 100\n","plt.hist(train[train.target == 0]['pun_counts'], alpha = 0.6, bins=bins, label='Fake', color='green')\n","plt.hist(train[train.target == 1]['pun_counts'], alpha = 0.6, bins=bins, label='Real', color='red')\n","plt.xlabel('punctuations counts')\n","plt.ylabel('count')\n","plt.xlim(0,40)\n","plt.legend(loc='upper right')"]},{"cell_type":"markdown","metadata":{},"source":["# Step 3 : clean up the tweets\n","\n","Get rid of :\n","* Links / html tags\n","\n","* Punctuation\n","\n","* Emojis - perhaps in other model, might be helpful, but in this one, ignore\n","\n","* Mentions\n","\n","* Stopwords"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import regex as re\n","def remove_tag(text):\n","    tag =re.compile(r'<(.*?)>')\n","    return tag.sub(r'',text)\n","\n","train['cleaned']=train['text'].apply(lambda x : remove_tag(x))\n","test['cleaned']=test['text'].apply(lambda x : remove_tag(x))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def remove_link(text):\n","    # all links start with 'http://t.co/' or 'https://t.co/'\n","    # the ? means 0 or 1, so s is optional in https\n","    link=re.compile(r\"https?:\\/\\/t.co\\/[A-Za-z0-9]+\")\n","    return link.sub(r'',text)\n","\n","train['cleaned']=train['cleaned'].apply(lambda x : remove_link(x))\n","test['cleaned']=test['cleaned'].apply(lambda x : remove_link(x))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def remove_mention(text):\n","    mention=re.compile(r\"@[A-Za-z0-9_]+[ :]\")\n","    return mention.sub(r'',text)\n","\n","train['cleaned']=train['cleaned'].apply(lambda x : remove_mention(x))\n","test['cleaned']=test['cleaned'].apply(lambda x : remove_mention(x))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# testing the functions to remove link and tags\n","word = remove_link(\"word a b c <a>http://t.co/abdcd</a> hello @aria_ahrary @TheTawniest:hello\")\n","word = remove_mention(word)\n","word = remove_tag(word)\n","\n","print(word)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# code taken from https://stackoverflow.com/questions/33404752/removing-emojis-from-a-string-in-python\n","def remove_emoji(text):\n","    emoji_pattern = re.compile(\"[\"\n","        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n","        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n","        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n","        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n","                           \"]+\", flags=re.UNICODE)\n","    return emoji_pattern.sub(r'', text)\n","\n","train['cleaned']=train['cleaned'].apply(lambda x : remove_emoji(x))\n","test['cleaned']=test['cleaned'].apply(lambda x : remove_emoji(x))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# testing the functions to remove emoji\n","word = remove_emoji(\"Haha ðŸ˜‚\")\n","print(word)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def remove_punct(text):\n","    table=str.maketrans('','',string.punctuation)\n","    return text.translate(table)\n","train['cleaned']=train['cleaned'].apply(lambda x : remove_punct(x))\n","test['cleaned']=test['cleaned'].apply(lambda x : remove_punct(x))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# testing the functions to remove punctuation\n","word = remove_punct(\"Hello. I am having a great time.\")\n","print(word)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def remove_stopword(text):\n","    for word in STOPWORDS:\n","            token = \" \" + word + \" \"\n","            text = text.replace(token, \" \")\n","            text = text.replace(\"  \", \" \")\n","    return text\n","\n","train['cleaned']=train['cleaned'].apply(lambda x : remove_stopword(x))\n","test['cleaned']=test['cleaned'].apply(lambda x : remove_stopword(x))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# testing the functions to remove punctuation\n","word = remove_stopword(\"Hello. I am having a great time.\")\n","print(word)"]},{"cell_type":"markdown","metadata":{},"source":["Looking at the cleaned text"]},{"cell_type":"markdown","metadata":{},"source":["* Looks like by removing punctuations, we are removing the dash in between words too, not ideal"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["train.tail()\n","# 7613 x 10"]},{"cell_type":"markdown","metadata":{},"source":["* We would want all the words to be lower case before tokenizing them, but let's just try TensorFlow tokenizer first\n","    * automatically remove punctuation\n","    * automatically lower case words"]},{"cell_type":"markdown","metadata":{},"source":["Let's split the training data into training and validation"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["train_labels = np.array(train['target'])\n","print(train_labels)\n","\n","#tweets = train['cleaned']\n","#tweets_test = test['cleaned']\n","\n","tweets = train['text']\n","tweets_test = test['text']"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","tweets_train, tweets_valid, labels_train, labels_valid = train_test_split(tweets, train_labels, test_size = 0.2, shuffle=True, random_state=0)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["print(\"tweets_train shape:\", tweets_train.shape)\n","print(\"labels_train shape:\", labels_train.shape)\n","\n","print(\"tweets_valid shape:\", tweets_valid.shape)\n","print(\"labels_valid shape:\", labels_valid.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["tweets.head()"]},{"cell_type":"markdown","metadata":{},"source":["# Step 4: Selecting the model to use and getting \n","## TensorFlow Keras Tokenizer"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","\n","vocab_size = 1000\n","embedding_dim=8\n","max_length = 27\n","num_epochs = 10\n","token = \"<Unknown>\"\n","\n","\n","# num_words=vocab_size\n","# oov_token=\"<Unknown>\"\n","tokenizer = Tokenizer(num_words=vocab_size,oov_token=token)\n","tokenizer.fit_on_texts(tweets_train)\n","word_index = tokenizer.word_index\n","\n","training_sequences = tokenizer.texts_to_sequences(tweets_train)\n","training_padded = pad_sequences(training_sequences, truncating = 'post', padding='post', maxlen=max_length)\n","\n","valid_sequences = tokenizer.texts_to_sequences(tweets_valid)\n","valid_padded = pad_sequences(valid_sequences, truncating = 'post', padding='post', maxlen=max_length)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["print(\"training_padded shape:\",training_padded.shape)\n","print(\"valid_padded shape:\",valid_padded.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["print(len(word_index))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import tensorflow as tf\n","model = tf.keras.Sequential([\n","    # make word embedding\n","    tf.keras.layers.Embedding(vocab_size,embedding_dim, input_length=max_length),\n","    #flatten the network or use pooling\n","    tf.keras.layers.Flatten(),\n","    # dense neural network\n","    tf.keras.layers.Dense(6,activation='relu'),\n","    tf.keras.layers.Dense(1,activation='sigmoid')\n","])\n","\n","model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n","model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["history = model.fit(training_padded, labels_train, epochs=num_epochs, validation_data=(valid_padded,labels_valid))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def plot_loss_and_accuracy(history, val,nrows,ncols,index):\n","    plt.subplot(nrows,ncols,index)\n","    plt.plot(history.history[val])\n","    plt.plot(history.history['val_'+val])\n","    plt.xlabel(\"# of Epochs\")\n","    plt.ylabel(val)\n","    plt.legend([val, \"val_\"+val])\n","\n","fig5, axes = plt.subplots(1,2, figsize=(10, 4), dpi=100)\n","plot_loss_and_accuracy(history,\"accuracy\",1,2,1)\n","plot_loss_and_accuracy(history,\"loss\",1,2,2)\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["Let's try some LSTM"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["vocab_size = 500\n","embedding_dim=16\n","max_length = 27\n","num_epochs = 10\n","token = \"<Unknown>\"\n","\n","\n","# num_words=vocab_size\n","# oov_token=\"<Unknown>\"\n","tokenizer = Tokenizer(num_words=vocab_size,oov_token=token)\n","tokenizer.fit_on_texts(tweets_train)\n","word_index = tokenizer.word_index\n","\n","training_sequences = tokenizer.texts_to_sequences(tweets_train)\n","training_padded = pad_sequences(training_sequences, truncating = 'post', padding='post', maxlen=max_length)\n","\n","valid_sequences = tokenizer.texts_to_sequences(tweets_valid)\n","valid_padded = pad_sequences(valid_sequences, truncating = 'post', padding='post', maxlen=max_length)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["model_LSTM = tf.keras.Sequential([\n","    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length= max_length),\n","    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n","    tf.keras.layers.Dense(24,activation='relu'),\n","    tf.keras.layers.Dense(1,activation='sigmoid')\n","])\n","model_LSTM.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n","history_LSTM = model.fit(training_padded, labels_train, epochs=num_epochs, validation_data=(valid_padded,labels_valid))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["fig6, axes = plt.subplots(1,2, figsize=(10, 4), dpi=100)\n","plot_loss_and_accuracy(history_LSTM,\"accuracy\",1,2,1)\n","plot_loss_and_accuracy(history_LSTM,\"loss\",1,2,2)\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["Let's try GRU"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["model_GRU = tf.keras.Sequential([\n","    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length= max_length),\n","    tf.keras.layers.Bidirectional(tf.keras.layers.GRU(32)),\n","    tf.keras.layers.Dense(6,activation='relu'),\n","    tf.keras.layers.Dense(1,activation='sigmoid')\n","])\n","model_GRU.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n","history_GRU = model.fit(training_padded, labels_train, epochs=num_epochs, validation_data=(valid_padded,labels_valid))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["fig7, axes = plt.subplots(1,2, figsize=(10, 4), dpi=100)\n","plot_loss_and_accuracy(history_GRU,\"accuracy\",1,2,1)\n","plot_loss_and_accuracy(history_GRU,\"loss\",1,2,2)\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["With 1D Convolutional network"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["model_Conv1D = tf.keras.Sequential([\n","    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length= max_length),\n","    tf.keras.layers.Conv1D(128,5,activation='relu'),\n","    tf.keras.layers.GlobalAveragePooling1D(),\n","    tf.keras.layers.Dense(6,activation='relu'),\n","    tf.keras.layers.Dense(1,activation='sigmoid')\n","])\n","model_Conv1D.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n","history_Conv1D = model.fit(training_padded, labels_train, epochs=num_epochs, validation_data=(valid_padded,labels_valid))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["fig8, axes = plt.subplots(1,2, figsize=(10, 4), dpi=100)\n","plot_loss_and_accuracy(history_Conv1D,\"accuracy\",1,2,1)\n","plot_loss_and_accuracy(history_Conv1D,\"loss\",1,2,2)\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["with sub_words"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["#import tensorflow_datasets as tfds\n","\n","#tokenizer = tfds.features.text.SubwordTextEncoder.build_from_corpus((en.numpy() for text in tweets_train), target_vocab_size = 2**10)"]},{"cell_type":"markdown","metadata":{},"source":["# Step 5 : tuning the hyperparamters\n","* changing the following:\n","    * vocab_size = 1000\n","    * embedding_dim=16\n","    * max_length = 27\n","    * num_epochs = 30"]},{"cell_type":"markdown","metadata":{},"source":["* Let's try different vocab_size"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["vocab_sizes = [250,500,1000,2000,5000]\n","embedding_dim=16\n","max_length = 27\n","num_epochs = 10\n","token = \"<Unknown>\"\n","\n","models=[]\n","\n","for vocab_size in vocab_sizes:\n","    tokenizer = Tokenizer(num_words=vocab_size,oov_token=token)\n","    tokenizer.fit_on_texts(tweets_train)\n","    word_index = tokenizer.word_index\n","\n","    training_sequences = tokenizer.texts_to_sequences(tweets_train)\n","    training_padded = pad_sequences(training_sequences, truncating = 'post', padding='post', maxlen=max_length)\n","\n","    valid_sequences = tokenizer.texts_to_sequences(tweets_valid)\n","    valid_padded = pad_sequences(valid_sequences, truncating = 'post', padding='post', maxlen=max_length)\n","\n","    model = tf.keras.Sequential([\n","        # make word embedding\n","        tf.keras.layers.Embedding(vocab_size,embedding_dim, input_length=max_length),\n","        #flatten the network or use pooling\n","        tf.keras.layers.Flatten(),\n","        # dense neural network\n","        tf.keras.layers.Dense(6,activation='relu'),\n","        tf.keras.layers.Dense(1,activation='sigmoid')\n","    ])\n","\n","    model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n","\n","    models.append(model.fit(training_padded, labels_train, epochs=num_epochs, validation_data=(valid_padded,labels_valid),verbose=0))\n","    print(\"Done with a round\")\n","\n","print(\"All Done\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["\n","fig6, axes = plt.subplots(5,2, figsize=(10, 15), dpi=100)\n","for i in range(len(models)):\n","    plot_loss_and_accuracy(models[i],\"accuracy\",5,2,(i+(i+1)))\n","    plot_loss_and_accuracy(models[i],\"loss\",5,2,(i+(i+2)))\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["vocab_sizes = 500\n","embedding_dims= [16,32]\n","max_length = 27\n","num_epochs = 10\n","token = \"<Unknown>\"\n","\n","models=[]\n","\n","for embedding_dim in embedding_dims:\n","    tokenizer = Tokenizer(num_words=vocab_size,oov_token=token)\n","    tokenizer.fit_on_texts(tweets_train)\n","    word_index = tokenizer.word_index\n","\n","    training_sequences = tokenizer.texts_to_sequences(tweets_train)\n","    training_padded = pad_sequences(training_sequences, truncating = 'post', padding='post', maxlen=max_length)\n","\n","    valid_sequences = tokenizer.texts_to_sequences(tweets_valid)\n","    valid_padded = pad_sequences(valid_sequences, truncating = 'post', padding='post', maxlen=max_length)\n","\n","    model = tf.keras.Sequential([\n","        # make word embedding\n","        tf.keras.layers.Embedding(vocab_size,embedding_dim, input_length=max_length),\n","        #flatten the network or use pooling\n","        tf.keras.layers.Flatten(),\n","        # dense neural network\n","        tf.keras.layers.Dense(6,activation='relu'),\n","        tf.keras.layers.Dense(1,activation='sigmoid')\n","    ])\n","\n","    model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n","\n","    models.append(model.fit(training_padded, labels_train, epochs=num_epochs, validation_data=(valid_padded,labels_valid),verbose=0))\n","    print(\"Done with a round\")\n","\n","print(\"All Done\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["\n","fig7, axes = plt.subplots(2,2, figsize=(10, 10), dpi=100)\n","for i in range(len(models)):\n","    plot_loss_and_accuracy(models[i],\"accuracy\",2,2,(i+(i+1)))\n","    plot_loss_and_accuracy(models[i],\"loss\",2,2,(i+(i+2)))\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{},"source":["* Let's settle on a final model"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["vocab_size = 1000\n","embedding_dim=8\n","max_length = 27\n","num_epochs = 4\n","token = \"<Unknown>\"\n","\n","tokenizer = Tokenizer(num_words=vocab_size,oov_token=token)\n","tokenizer.fit_on_texts(tweets)\n","word_index = tokenizer.word_index\n","\n","all_sequences = tokenizer.texts_to_sequences(tweets)\n","all_padded = pad_sequences(all_sequences, truncating = 'post', padding='post', maxlen=max_length)\n","\n","test_sequences = tokenizer.texts_to_sequences(tweets_test)\n","test_padded = pad_sequences(test_sequences, truncating = 'post', padding='post', maxlen=max_length)\n","\n","model = tf.keras.Sequential([\n","        # make word embedding\n","    tf.keras.layers.Embedding(vocab_size,embedding_dim, input_length=max_length),\n","        #flatten the network or use pooling\n","    tf.keras.layers.Flatten(),\n","        # dense neural network\n","    tf.keras.layers.Dense(6,activation='relu'),\n","    tf.keras.layers.Dense(1,activation='sigmoid')\n","])\n","\n","model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n","\n","final = (model.fit(all_padded, train_labels, epochs=num_epochs, verbose=2))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["prediction = model.predict(test_padded)"]},{"cell_type":"markdown","metadata":{},"source":["# Step 6 : get the output file"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["prediction_flatten = np.ravel(prediction) # flatten it in order to make into a series in a dataframe"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["submission = pd.DataFrame({\n","        \"id\": test.index,\n","        \"target\": prediction_flatten\n","    })\n","\n","def threshold(val):\n","    if val >= 0.5:\n","        value = 1\n","    else:\n","        value = 0\n","    return value\n","submission['target'] = submission['target'].apply(threshold)\n","\n","submission.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["submission.to_csv('submission.csv', index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}